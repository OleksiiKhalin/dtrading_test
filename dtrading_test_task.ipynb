{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# D.Trading test task\n",
    "## Oleksii Khalin\n",
    "\n",
    "To fulfill the task, I decided to create following DB structure (tables):\n",
    "- base_year\n",
    "- base_quarter\n",
    "- base_month\n",
    "- base_week\n",
    "- base_weekend\n",
    "- base_day\n",
    "- peak_year\n",
    "- peak_quarter\n",
    "- peak_month\n",
    "- peak_week\n",
    "- peak_weekend\n",
    "- peak_day-\n",
    "\n",
    "I could have also done it in 6 or even 1 table(s), but it is always better to keep data separately\n",
    "\n",
    "What does the following code do?\n",
    "    \n",
    "It connects to the page https://www.eex.com/en/market-data/power/futures#%7B%22snippetpicker%22%3A%2228%22%7D and parses data for the last N amount of days (20 as default) not including today.\n",
    "\n",
    "Code has the following structure:\n",
    "\n",
    "- Import block\n",
    "- Date list definition\n",
    "- Dictionaries definition\n",
    "- Request function\n",
    "- Connection to DB\n",
    "- \"Future\" column interpreter function\n",
    "- Parse and upload section\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import block\n",
    "\n",
    "Gets all the neccessary libraries to build up code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import datetime as datetime\n",
    "from datetime import date, timedelta, datetime\n",
    "import _strptime\n",
    "import math as math\n",
    "import sqlite3 as sqlite3\n",
    "import psycopg2 as psycopg2\n",
    "import sqlalchemy as sqlalchemy\n",
    "from sqlalchemy import create_engine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Date list definition\n",
    "\n",
    "Creates datelist which contains N number of dates from today, not including today"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = date.today() - timedelta(1) \n",
    "\n",
    "#change the value in range to alter number of days\n",
    "Dateslist = [(today - timedelta(days = day)) for day in range(25)]\n",
    "# https://stackoverflow.com/questions/64649848/how-to-get-the-last-20-days-dates-in-the-form-of-list-in-python\n",
    "# https://www.programiz.com/python-programming/datetime/strftime\n",
    "\n",
    "Dateslist = Dateslist[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries definition\n",
    "\n",
    "Creates dictionaries and lists which are used later in the code to parse data to DB tables from approprite website page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates a dictionary with timeframe data which is then used in requests\n",
    "base_timeframe_dict = {\n",
    "    \"year\":     \"/E.DEBY\",\n",
    "    \"quarter\":  \"/E.DEBQ\",\n",
    "    \"month\":    \"/E.DEBM\",\n",
    "    \"week\":     \"/E.DEB_WEEK\",\n",
    "    \"weekend\":  \"/E.DWB_WEEK\",\n",
    "    \"day\":      \"/E.DB_DAILY\",\n",
    "}\n",
    "peak_timeframe_dict = {\n",
    "    \"year\":     \"/E.DEPY\",\n",
    "    \"quarter\":  \"/E.DEPQ\",\n",
    "    \"month\":    \"/E.DEPM\",\n",
    "    \"week\":     \"/E.DEP_WEEK\",\n",
    "    \"weekend\":  \"/E.DWP_WEEK\",\n",
    "    \"day\":      \"/E.DP_DAILY\",\n",
    "}\n",
    "\n",
    "# define timeframe to cover in parsing\n",
    "timeframe_list = [\"year\", \"quarter\", \"month\", \"week\", \"weekend\", \"day\"]\n",
    "# timeframe_list = [\"month\", \"week\"]\n",
    "\n",
    "# define type of table to be used in the loop\n",
    "type_list = [\"base\", \"peak\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Request function\n",
    "\n",
    "Function which calls appropriate page from the website and exports request, which is then converted to json and data is parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Go to website, click  and copy cURL(bash) of appropriate network request\n",
    "# Then go to https://curlconverter.com/ and convert to Python\n",
    "# Paste here the results\n",
    "\n",
    "def makerequest(type, timeframe, date):\n",
    "    \n",
    "    optionroot = ''\n",
    "    if type == 'base': optionroot = base_timeframe_dict[timeframe]\n",
    "    if type == 'peak': optionroot = peak_timeframe_dict[timeframe]\n",
    "    \n",
    "    headers = {\n",
    "        'Accept': '*/*',\n",
    "        'Accept-Language': 'en,en-US;q=0.9,ru-RU;q=0.8,ru;q=0.7',\n",
    "        'Connection': 'keep-alive',\n",
    "        'DNT': '1',\n",
    "        'Origin': 'https://www.eex.com',\n",
    "        'Referer': 'https://www.eex.com/',\n",
    "        'Sec-Fetch-Dest': 'empty',\n",
    "        'Sec-Fetch-Mode': 'cors',\n",
    "        'Sec-Fetch-Site': 'cross-site',\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/107.0.0.0 Safari/537.36',\n",
    "        'sec-ch-ua': '\"Google Chrome\";v=\"107\", \"Chromium\";v=\"107\", \"Not=A?Brand\";v=\"24\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"Windows\"',\n",
    "    }\n",
    "\n",
    "    params = {\n",
    "        'optionroot': f'\"{optionroot}\"',\n",
    "        'expirationdate': f'{(date - timedelta(days=1)).strftime(\"%Y/%m/%d\")}',\n",
    "        'onDate': f'{date.strftime(\"%Y/%m/%d\")}', # inserts the list value with certain date we export data from\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://webservice-eex.gvsi.com/query/json/getChain/gv.pricesymbol/gv.displaydate/gv.expirationdate/tradedatetimegmt/gv.eexdeliverystart/ontradeprice/close/onexchsingletradevolume/onexchtradevolumeeex/offexchtradevolumeeex/openinterest/', params=params, headers=headers)\n",
    "\n",
    "    return response\n",
    "    # print(f\"response {Dateslist.index(date)} is successful {response}\")\n",
    "    # # checks the success\n",
    "    # [200] means request is successful\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connection to DB\n",
    "\n",
    "Uses alchemy SQL library in order to make connection to PostgreSQL DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert database username, password, host, name of yours to upload the data\n",
    "conn = create_engine('postgresql+psycopg2://postgres:7117875@localhost/dtrading_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Future\" column interpreter function\n",
    "\n",
    "Converts dates from json query into appropriate output similar to one in the origin website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makefuture(timeframe, display_date, deliverystart_date):\n",
    "    if timeframe == \"year\":\n",
    "        return f\"Call-{datetime.strptime(display_date,'%m/%d/%Y').strftime('%Y')[2:]}\"\n",
    "    elif timeframe == \"quarter\":\n",
    "        return f\"{math.ceil(int(datetime.strptime(display_date,'%m/%d/%Y').strftime('%m'))/3)}/{datetime.strptime(display_date,'%m/%d/%Y').strftime('%Y')[2:]}\"\n",
    "    elif timeframe == \"month\":\n",
    "        return f\"{datetime.strptime(display_date,'%m/%d/%Y').strftime('%B')[:3]}/{datetime.strptime(display_date,'%m/%d/%Y').strftime('%Y')[2:]}\"\n",
    "    elif timeframe == 'week':\n",
    "        return f\"Week {datetime.strptime(display_date,'%m/%d/%Y').strftime('%W')}/{datetime.strptime(display_date,'%m/%d/%Y').strftime('%Y')[2:]}\"\n",
    "    elif timeframe == \"weekend\":\n",
    "        return f\"WkEnd {datetime.strptime(display_date,'%m/%d/%Y').strftime('%d/%m')}\"\n",
    "    elif timeframe == \"day\":\n",
    "        return datetime.strptime(deliverystart_date[:-3],'%m/%d/%Y %H:%M:%S').strftime(\"%m/%d/%Y\")\n",
    "    else: return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse and upload section\n",
    "\n",
    "Core section responsible for loops, which upload data to lists, which are then combined into pandas dataframes and uploaded to the DB.\n",
    "\n",
    "Code also checks whether the same data was uploaded previously to avoid dublicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "#* create lists to insert data into database\n",
    "base_date = []\n",
    "base_future = []\n",
    "base_last_price = []\n",
    "base_last_volume = []\n",
    "base_settlement_price = []\n",
    "base_volume_exchange = []\n",
    "base_volume_trade_registration = []\n",
    "base_open_interest = [] \n",
    "\n",
    "for type in type_list:\n",
    "\n",
    "    for timeframe in timeframe_list:   \n",
    "\n",
    "        for date in Dateslist:\n",
    "\n",
    "            response = makerequest(type, timeframe, date)\n",
    "            \n",
    "            result_json = response.json() # gets response in json format\n",
    "\n",
    "            # result_json.keys() - used to check all the keys\n",
    "            # len(result_json['results']['items']) - checks the length of dictionary (number of entries)\n",
    "\n",
    "            result_items = result_json['results']['items'] # gets one level indide the responce dictionary\n",
    "\n",
    "            for items in result_items:             \n",
    "                \n",
    "                base_date.append(date) \n",
    "                try:\n",
    "                    base_future.append(makefuture(timeframe, items['gv.displaydate'], items['gv.eexdeliverystart']))\n",
    "                except:\n",
    "                    base_future.append(None)\n",
    "                try:\n",
    "                    base_last_price.append(items['ontradeprice'])\n",
    "                except:\n",
    "                    base_last_price.append(None)\n",
    "                try:\n",
    "                    base_last_volume.append(items['onexchsingletradevolume'])\n",
    "                except:\n",
    "                    base_last_volume.append(None)\n",
    "                try:\n",
    "                    base_settlement_price.append(items['close'])\n",
    "                except:\n",
    "                    base_settlement_price.append(None)\n",
    "                try:\n",
    "                    base_volume_exchange.append(items['onexchtradevolumeeex'])\n",
    "                except:\n",
    "                    base_volume_exchange.append(None)\n",
    "                try:\n",
    "                    base_volume_trade_registration.append(items['offexchtradevolumeeex'])\n",
    "                except:\n",
    "                    base_volume_trade_registration.append(None)\n",
    "                try:\n",
    "                    base_open_interest.append(items['openinterest'])\n",
    "                except:\n",
    "                    base_open_interest.append(None)\n",
    "                    \n",
    "            # creates pandas DF\n",
    "            df_base = pd.DataFrame({\n",
    "                'date': base_date, \n",
    "                'future': base_future,\n",
    "                'last_price': base_last_price, \n",
    "                'last_volume': base_last_volume, \n",
    "                'settlement_price': base_settlement_price, \n",
    "                'volume_exchange': base_volume_exchange, \n",
    "                'volume_trade_registration': base_volume_trade_registration, \n",
    "                'open_interest': base_open_interest})\n",
    "\n",
    "            # following code checks whether we already have data for some dates in database\n",
    "            # and removes that from dataframe which is about to be uploaded\n",
    "            try:\n",
    "                query_dates = f'SELECT DISTINCT date AS date FROM public.{type}_{timeframe}'\n",
    "                df_existing_dates = pd.read_sql_query(con=conn, sql=query_dates)\n",
    "                if df_existing_dates.empty:\n",
    "                    list_of_dates = []\n",
    "                else:\n",
    "                    list_of_dates = list(df_existing_dates.date)\n",
    "                df_base = df_base[~df_base['date'].isin(list_of_dates)]\n",
    "            except:\n",
    "                print(\"data is uploaded for the first time\")\n",
    "            \n",
    "            # uploades data to database by using to_sql method\n",
    "            df_base.to_sql(f'{type}_{timeframe}', con=conn, if_exists='append', index=False)\n",
    "\n",
    "            # drops lists and dataframe in order to avoid dublicates in the following cycle\n",
    "            base_date = []\n",
    "            base_future = []\n",
    "            base_last_price = []\n",
    "            base_last_volume = []\n",
    "            base_settlement_price = []\n",
    "            base_volume_exchange = []\n",
    "            base_volume_trade_registration = []\n",
    "            base_open_interest = []\n",
    "            df_base.drop(df_base.index,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trash code to use in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fast drop tables in DB\n",
    "# DROP TABLE IF EXISTS public.base_day;\n",
    "# DROP TABLE IF EXISTS public.base_month;\n",
    "# DROP TABLE IF EXISTS public.base_quarter;\n",
    "# DROP TABLE IF EXISTS public.base_week;\n",
    "# DROP TABLE IF EXISTS public.base_weekend;\n",
    "# DROP TABLE IF EXISTS public.base_year;\n",
    "# DROP TABLE IF EXISTS public.peak_day;\n",
    "# DROP TABLE IF EXISTS public.peak_month;\n",
    "# DROP TABLE IF EXISTS public.peak_quarter;\n",
    "# DROP TABLE IF EXISTS public.peak_week;\n",
    "# DROP TABLE IF EXISTS public.peak_weekend;\n",
    "# DROP TABLE IF EXISTS public.peak_year\n",
    " \n",
    " # conn.execute(f'''\n",
    "            #     CREATE TABLE IF NOT EXISTS {type}_{timeframe} (\n",
    "            #         key                         INTEGER PRIMARY KEY NOT NULL,\n",
    "            #         date                        DATE, \n",
    "            #         future                      TEXT,\n",
    "            #         last_price                  FLOAT,\n",
    "            #         last_volume                 FLOAT,\n",
    "            #         settlement_price            FLOAT,\n",
    "            #         volume_exchange             FLOAT,\n",
    "            #         volume_trade_registration   FLOAT,\n",
    "            #         open_interest               FLOAT\n",
    "            #     )\n",
    "            # ''')\n",
    "\n",
    "#base_relative_date = [] # date which is used to make appropriate string in future column\n",
    "            ## by request additional data can be parsed:\n",
    "            # gv_pricesymbol = [] \n",
    "            # gv_displaydate = []\n",
    "            # gv_expirationdate = []\n",
    "            # tradedatetimegmt = []\n",
    "            # gv_eexdeliverystart = []\n",
    "\n",
    "#*additional data can be parsed\n",
    "                # try:\n",
    "                #     gv_pricesymbol = ['gv.pricesymbol']\n",
    "                # except:\n",
    "                #     gv_pricesymbol.append(None)\n",
    "                # try:\n",
    "                #     base_relative_date.append(items['gv.displaydate'])\n",
    "                # except:\n",
    "                #     base_relative_date.append(None)\n",
    "                # try:\n",
    "                #     gv_expirationdate.append(items['gv.expirationdate'])\n",
    "                # except:\n",
    "                #     gv_expirationdate.append(None)\n",
    "                # try:\n",
    "                #     tradedatetimegmt.append(items['tradedatetimegmt'])\n",
    "                # except:\n",
    "                #     tradedatetimegmt.append(None)\n",
    "                # try:\n",
    "                #     gv_eexdeliverystart.append(items['gv.eexdeliverystart'])\n",
    "                # except:\n",
    "\n",
    "# df_base.to_sql('wrk_upsert', con=conn, if_exists='replace', index=False)\n",
    "            \n",
    "            # conn.execute(f'''\n",
    "            #     INSERT INTO {type}_{timeframe} (date, future, last_price, last_volume, settlement_price, volume_exchange, volume_trade_registration, open_interest)\n",
    "            #         SELECT (date, future, last_price, last_volume, settlement_price, volume_exchange, volume_trade_registration, open_interest) \n",
    "            #         FROM wrk_upsert\n",
    "            #         ON CONFLICT (date)\n",
    "            #             DO NOTHING\n",
    "            # ''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6f7ab3e478e3b129be59d5337de54112cb0fd0d01466b14d8f23e39a2c59c52c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
